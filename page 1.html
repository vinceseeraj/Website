<!DOCTYPE html>
<html>
<body style="background-color:black"> 
<style>	
img {
  max-width: 100%;
  height: auto;
  width: auto;
}
p{
  max-width: 800px;
  min-width: 100px;
}
	</style>
  <head>
    <link rel="stylesheet" href="main.css">
    <title>Window Title</title> 
  </head>

  <body>
  <div class="header">
    <h1>Voice Recognition</h1>
	<h2>Techniques</h2>
    <div class="nav">
        <ul>
          <li><a href="index.html">Abstract</a></li>
		  <li><a href="page 1.html">Techniques</a></li>
		  <li><a href="page 2.html">Codes</a></li>
          <li><a href="othercontent.html">References</a></li>
        </ul>
    </div>
  </div>

<div class="bottom">

<div class="side-bar">
 
  <h2 style="color:white">Pages</h2>
  <div class="side-nav">
    <ul>
	  <li><a href="index.html" style="color:white">Abstract</a></li>
	  <li><a href="page 1.html" style="color:white">Techniques</a></li>
	  <li><a href="page 2.html" style="color:white">Codes</a></li>
	  <li><a href="othercontent.html" style="color:white">References</a></li>
    </ul>
  </div>
</div>

<div class="container">
<h1 style="color:white">Mel-Frequency Cepstrum Coefficients Processor</h1>
<p style="color:white;text-indent: 40px" align="justify">Speaker recognition is the process of automatically recognizing who is speaking based on individual information included in speech waves. 
 This technique makes it possible to use the speaker's voice to verify their identity and control access to services such as voice dialing, banking by telephone, telephone shopping, database access services, information services, voice mail, security control for confidential information areas, and remote access to computers. 
 Speech input is typically recorded at a sampling rate of above 10,000 Hz. This sampling frequency was chosen to minimize the effects of aliasing during analog-to-digital conversion. 
<p style="color:white;text-indent: 40px" align="justify">These sampled signals can capture all frequencies up to 5 kHz, which cover most energy of sounds that are generated by humans. The main purpose of the MFCC processor is to mimic the behavior of the human ears. 
	MFCC’s are based on the known variation of the human ear’s critical bandwidths with frequency, filters spaced linearly at low frequencies and logarithmically at high frequencies have been used to capture the phonetically important characteristics of speech. Also, MFFC’s are known to be less susceptible to variations such as a person's voice when sick compared to their regular healthy voice. 
	The 5 techniques depicted will be shown in more detail in their respective sections. In regard to our code, the relevant DSP techniques will be noted in the comments of our voice recognition algorithm.
</p>

 <figure class="right">
  <img src="Picture6.png" alt="mask" height="400" width="600">
	<p style="color:white" align="justify">
		Figure 1: MFCC Steps for <sup>4</sup>
	</p>
  </figure> 
	
<h1 style="color:white">Frame Blocking</h1>
<p style="color:white;text-indent: 40px" align="justify">The following graph is used to explain frame blocking and is not a reflection of our recorded data. The input signal, which is our recorded voices, is blocked into frames of N samples with
N having a typical value of 256 samples. 
Now the reason for this is, if the samples are low like at 128 samples, then during training, it will have issues identifying distortion due to noninteger values. 
This means that depending on the frequency of the samples, it will give this issue. On the other hand for high sampling values, such as 512 samples,
the samples incorrectly sample the amount needed for the detection of voice. Thus, when actual testing occurs, there needs to be more training on the system for accuracy. 
More specifically, the quantity of recorded voice determines the appropriate samples to be used. In our case, we want to verify a person's identity over the phone when muttering a password that is usually short in length. Thus, 256 samples was appropriate for our case.
The Frames are then separated by a Frame Number, M 
with the typical value of M being 100 Frames.</p>
 <figure class="right">	
  <img src="Picture1.png" alt="frame" height="400" width="600">
	<p style="color:white" align="justify">
		Figure 2: Frame Blocking for 250 samples of speech vs Frame number<sup>1</sup>
	</p>
</figure>
<h1 style="color:white">Windowing</h1>

<p style="color:white;text-indent: 40px" align="justify">
Windowing is a finite length of a sequence, that is formed from an infinite length sequence by multiplying the infinite length sequence by a window. 
	This technique is used to reduce the effects of truncation of a wave from FFT over a noninteger number of cycles. 
	When the FFT has these issues, it causes the energy of the frequency to leak into another frequency causing spectral frequency. 
	As opposed to when windowing is applied, we get the real spectrum of the given waveform and reduce the spectral frequency error of FFT.
	
<p style="color:white;text-indent: 40px" align="justify">Each frame is windowed in this step and
this process assists in eliminating spectral distortion and noise.
The idea here is to set the beginning and end of each frame to 0 so that the discontinuities can be eliminated.
We define the window as w(n), in the specified interval shown, where N is the number of samples in each frame, then the result of windowing is the signal.
A typical windowing process is the Hamming window, the reason for this is that they have a low impact on the amplitude accuracy for the frequency spectrum
as oppose to other windows such as rectangular or triangular.
	Shown below is the weight equation of the Hamming Window:</p>
 <figure class="right">	
  <img src="Picture2.png" alt="frame" height="400" width="600">
	<p style="color:white" align="justify">
		Figure 3: Window Equation<sup>1</sup>
	</p>
</figure>

<h1 style="color:white">Fast Fourier Transform (FFT)</h1>

<p style="color:white;text-indent: 40px" align="justify">The Fast Fourier Transform converts each frame of N samples from the time domain to the Frequency Domain.
The FFT is a fast algorithm to implement the Discrete Fourier Transform (DFT), which is defined on the set of N samples x sub n, as shown in the equation below.
Here, k = 0,1,2,...,N-1.
In general, X sub k are complex numbers and we only consider their absolute values (frequency magnitudes). For our purpose, frame blocking and windowing has been setup so that we are prepared to transition into the frequency domain during FFT.
</p>
	 <figure class="right">	
  <img src="Picture3.png" alt="frame" height="400" width="600">
	<p style="color:white" align="justify">
		Figure 4: Fast Fourier Transform Equation<sup>1</sup>
	</p>
</figure>
	
<h1 style="color:white"> Mel-Frequency Wrapping</h1>

<p style="color:white;text-indent: 40px" align="justify">One way to simulate the subjective spectrum is to use a filter bank, spaced uniformly on the Mel-scale. 
	That filter bank has a triangular bandpass frequency response, and the spacing, as well as the bandwidth, is determined by a constant Mel Frequency interval. 
	The number of mel spectrum coefficients, K, is typically chosen as 20.
 </p>
	 <figure class="right">	
  <img src="picture4.png" alt="frame" height="400" width="600">
	<p style="color:white" align="justify">
		Figure 5: Mel-Frequency Spaced Filterbank Wrapping for our own voice test saying hello.
	</p>
</figure>
	
<h1 style="color:white">Cepstrum</h1>
<p style="color:white;text-indent: 40px" align="justify">In the final step, we convert the log mel spectrum back to time.  The result is called the Mel Frequency Cepstrum Coefficients (MFCC).  The cepstral representation of the speech spectrum provides a good representation of the local spectral properties of the signal for the given frame analysis. 
These sets of coefficients are also called acoustic vectors. One thing to note about this conversion is that it brings the system close to human listening response since we do not hear on a linear scale. 
In turn, this helps the system be able to capture the phonetic characteristics of human speech. 
<p style="color:white;text-indent: 40px" align="justify">The problem of speaker recognition comes back to pattern recognition.  The goal of pattern recognition is to classify objects of interest into one of a number of categories or classes.  The objects of interest are generically called patterns and in our case are sequences of acoustic vectors that are extracted from an input speech using the 5 techniques that we described.
Because the mel spectrum coefficients are real numbers, we can convert them to the time domain using the Discrete Cosine Transform (DCT):
</p>
 <figure class="right">	
  <img src="Picture5.png" alt="frame" height="200" width="600">
	<p style="color:white" align="justify">
		Figure 6: Cepstrum Equation<sup>1</sup>
	</p>
</figure>

<h1 style="color:white">DATA</h1>
<p style="color:white;text-indent: 40px" align="justify">
	Graph
</p>
<figure class="right">	
  <img src="samplerate128.PNG" alt="frame" height="400" width="600">
	
  <img src="samplerate256.PNG" alt="frame" height="400" width="600">
       
  <img src="samplerate512.PNG" alt="frame" height="400" width="600">     
	
	<p style="color:white" align="justify">
		Figure 7: Frame Blocking for 3 different sampling rates
	</p>
</figure>
</body>
</html>
